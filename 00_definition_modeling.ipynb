{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM for Definition Modeling\n",
    "\n",
    "Definition modeling is a task where a language model is requested to generate a definition for a given word.\n",
    "In this notebook, we want to explore the ability of large language models to generate definitions for unknown words.\n",
    "As LLMs are trained on large corpora, it is possible that their training data contains obscure words, so no existing\n",
    "words can be used to test their ability. To address this issue, we will create several made up words and create example\n",
    "sentences with them. The models will have to generate a definition for these words based just on the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import json\n",
    "\n",
    "\n",
    "HF_TOKEN = None\n",
    "# Read file \"keys.json\" to get API keys\n",
    "with open('keys.json') as f:\n",
    "    keys = json.load(f)\n",
    "\n",
    "    # Set API keys\n",
    "    openai.api_key = keys['openai']\n",
    "    openai.organization = keys['openai-organization']\n",
    "    HF_TOKEN = keys['huggingface']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(before_examples, examples, after_examples):\n",
    "    joined_examples = \"\\n\".join(examples)\n",
    "    return f\"\"\"{before_examples}{joined_examples}{after_examples}\"\"\"\n",
    "\n",
    "def build_openai_completion(prompt, model=\"text-davinci-003\", temperature=0, max_tokens=300, top_p=1, frequency_penalty=0, presence_penalty=0):\n",
    "    response = openai.Completion.create(\n",
    "        prompt=prompt,\n",
    "        model=model,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "        top_p=top_p,\n",
    "        frequency_penalty=frequency_penalty,\n",
    "        presence_penalty=presence_penalty\n",
    "    )\n",
    "    return response[\"choices\"][0][\"text\"]\n",
    "\n",
    "def build_openai_chat(prompt, examples, model=\"gpt-3.5-turbo\"):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": prompt},\n",
    "            {\"role\": \"user\", \"content\": \"\\n\".join(examples)}\n",
    "        ]\n",
    "    )\n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query HuggingFace models\n",
    "import requests\n",
    "\n",
    "def hf_query(payload, model_id):\n",
    "\theaders = {\"Authorization\": f\"Bearer {HF_TOKEN}\"}\n",
    "\tAPI_URL = f\"https://api-inference.huggingface.co/models/{model_id}\"\n",
    "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
    "\tjson_response = None\n",
    "\ttry:\n",
    "\t\tjson_response = response.json()\n",
    "\t\treturn json_response\n",
    "\texcept:\n",
    "\t\tprint(f\"Error: {response}\")\n",
    "\t\treturn [{'generated_text': 'Error generating the definition'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads training data from words.json\n",
    "def load_training_data():\n",
    "    with open(\"words.json\") as f:\n",
    "        words = json.load(f)\n",
    "        return words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get definitions for all words in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 20\n"
     ]
    }
   ],
   "source": [
    "dataset = load_training_data()\n",
    "print(f\"Total words: {len(dataset)}\")\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_definitions(getter_func, dataset):\n",
    "    return {\n",
    "        w : getter_func(d[\"examples\"])\n",
    "        for w, d in tqdm(dataset.items())\n",
    "    }\n",
    "\n",
    "def save_definitions(definitions, filename):\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(definitions, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI completion models\n",
    "\n",
    "OPENAI_COMPLETION_BEFORE = \"Write a dictionary definition for the word in asterisks, given its context.\\nExamples:\\n\"\n",
    "OPENAI_COMPLETION_AFTER = \"\\nDefinition:\\n\"\n",
    "\n",
    "def openai_completion_helper(model, examples, **kwargs):\n",
    "    prompt = build_prompt(OPENAI_COMPLETION_BEFORE, examples, OPENAI_COMPLETION_AFTER)\n",
    "    return build_openai_completion(prompt, model=model, **kwargs)\n",
    "\n",
    "model_getter_func = {\n",
    "    \"text-davinci-003\": lambda examples: openai_completion_helper(\"text-davinci-003\", examples),\n",
    "    \"text-curie-001\": lambda examples: openai_completion_helper(\"text-curie-001\", examples),\n",
    "    \"text-babbage-001\": lambda examples: openai_completion_helper(\"text-babbage-001\", examples),\n",
    "    \"text-ada-001\": lambda examples: openai_completion_helper(\"text-ada-001\", examples)\n",
    "}\n",
    "\n",
    "for model_name, getter_func in model_getter_func.items():\n",
    "    definitions = get_definitions(getter_func, dataset)\n",
    "    save_definitions(definitions, f\"definitions-{model_name}.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_CHAT_PROMPT = \"\"\"You are a language researcher at the Real Academia Espa√±ola. Your job is to write dictionary definitions for words from its context. Many of those words have been recently discovered or created, so are not registered in any dictionary.\n",
    "\n",
    "Your responses must be formated like a dictionary definition.\n",
    "The response must be in Spanish.\n",
    "Itemize each found meaning and cluster similar meanings into a single definition.\n",
    "Instead of using the given examples, generate a new one of your own.\n",
    "Specify if the word is a verb, noun, adjective, adverb, etc.\n",
    "In your definition, do not write the word within asterisks.\n",
    "never mention facts that cannot be deduced from the examples given.\"\"\"\n",
    "\n",
    "def openai_chat_helper(examples, **kwargs):\n",
    "    return build_openai_chat(OPENAI_CHAT_PROMPT, examples, **kwargs)\n",
    "\n",
    "\n",
    "model_getter_func = {\n",
    "    \"gpt-3.5-turbo\": lambda examples: openai_chat_helper(examples),\n",
    "}\n",
    "\n",
    "for model_name, getter_func in model_getter_func.items():\n",
    "    definitions = get_definitions(getter_func, dataset)\n",
    "    save_definitions(definitions, f\"definitions-{model_name}.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'a small, secluded, or secret place'}]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_query({\n",
    "\t\"inputs\": build_prompt(OPENAI_COMPLETION_BEFORE, dataset[\"kiliche\"][\"examples\"], OPENAI_COMPLETION_AFTER),\n",
    "}, \"google/flan-t5-xxl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_COMPLETION_BEFORE = \"Write a dictionary definition for the word in asterisks, given its context.\\nExamples:\\n\"\n",
    "HF_COMPLETION_AFTER = \"\\nDefinition:\\n\"\n",
    "\n",
    "def hf_completion_helper(model, examples, **kwargs):\n",
    "    prompt = build_prompt(HF_COMPLETION_BEFORE, examples, HF_COMPLETION_AFTER)\n",
    "    response = hf_query({\"inputs\": prompt, \"wait_for_model\": True, \"max_new_tokens\": 250}, model)\n",
    "    print(response)\n",
    "    return response[0]['generated_text']\n",
    "\n",
    "model_getter_func = {\n",
    "    \"google-flan-t5-small\": lambda examples: hf_completion_helper(\"google/flan-t5-small\", examples),\n",
    "    \"google-flan-t5-base\": lambda examples: hf_completion_helper(\"google/flan-t5-base\", examples),\n",
    "    \"google-flan-t5-xxl\": lambda examples: hf_completion_helper(\"google/flan-t5-xxl\", examples),\n",
    "}\n",
    "\n",
    "for model_name, getter_func in model_getter_func.items():\n",
    "    print(model_name)\n",
    "    definitions = get_definitions(getter_func, dataset)\n",
    "    save_definitions(definitions, f\"definitions-{model_name}.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
